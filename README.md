I ran my code in Google Colab. I opened a new notebook, pasted the single-cell script from the **Code/** folder, and executed it. The cell prompted me to upload **initialraw\_dataset.xlsx**; once selected, it standardized headers to snake\_case, created **report\_date** as the last day of the month (first column), cleaned **jurisdiction** to “X County” (second column), coerced numeric fields (non-numeric → `NaN`), and normalized any `*date*` columns to `YYYY-mm-dd`. When the run finished, Colab downloaded the cleaned file, which I saved as **finalprocessed\_dataset.csv** (also written to `/content/processed`).

Assumptions & rationale: I assumed each file included a parseable reporting month and 4-digit year, that jurisdictions map to California counties even when labeled as agencies (e.g., “Sheriff’s Dept.”), and that columns containing words like “count/total/capacity/booking/release/adp” are numeric-intended. Issues I ran into included agency-style jurisdiction names, stray tokens like “U” and “N/A” in numeric fields, and mixed date formats (e.g., `M/D/YYYY`). To verify accuracy, I spot-checked that `report_date` always equals the month-end, confirmed `jurisdiction` ends with “County,” ensured all `*date*` columns export as ISO dates, validated that numeric-intended columns successfully coerce with non-numeric values becoming `NaN`, and compared row counts against the source to confirm no loss. I chose this approach to keep the pipeline generalizable across months, enforce strict typing for reliable analysis, and keep a consistent column order (`report_date`, `jurisdiction`, then everything else) to simplify downstream merges and QA.
